# -*- coding: utf-8 -*-
"""Untitled30.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mYd3TjYq0pkNeF_wNarZ4fPq_eferbJk
"""
import os
os.environ["OPENAI_API_KEY"] = "REPLACE_YOUR_KEY_HERE"

f = open("example.txt", "a")
f.write("""OpenAI o1 is a reasoning-first generative transformer that spends additional computation on internal chain-of-thought before producing a response, markedly improving performance on multi-step reasoning tasks in coding, mathematics, and scientific domains. First previewed on September 12, 2024 and fully released to ChatGPT users on December 5, 2024, o1 comes in multiple variants—o1-preview, o1-mini, and o1-pro—balancing cost, latency, and capability . By shifting focus from sheer scale to deliberative reasoning, o1 represents a new paradigm in LLM design, yielding PhD-level performance on scientific benchmarks and strong code-generation accuracy """)
f.close()


f1 = open("dockertutorial.txt", "a")
f1.write("""

docker stop <container_name_or_ID>
Gracefully stop a running container. Example: docker stop web1.
docker start <container_name_or_ID>
Restart a stopped container. Example: docker start web1.
docker restart <container_name_or_ID>
Stop and then immediately start a container. Example: docker restart web1.
docker rm <container_name_or_ID>
Remove a stopped container. Example: docker rm web2.
docker rm --force <container_name_or_ID>
Forcefully remove a running container (stops it if needed). Example: docker rm --force web3.

6. Interacting with Running Containers
docker exec -it <container_name_or_ID> bash
Open a new interactive shell session inside a running container. Example: docker exec -it oscontr1 bash.
docker attach <container_name_or_ID>
Attach your terminal to the main process of a running container. To detach without stopping, press Ctrl+P then Ctrl+Q.

7. Viewing Container Logs
docker logs <container_name_or_ID>
Retrieve the standard output and error logs from a container. Example: docker logs apache1.

8. Networking and IP Addressing
First, inspect to find a container’s IP:
docker inspect <container_name_or_ID> | grep IPAddress
Use the returned IP to access services inside the container, for example:
curl <container_IP>
curl <container_IP>/page.html

9. Building Custom Images with a Dockerfile
Create a directory and navigate into it:
mkdir helloworld
cd helloworld

Create a file named Dockerfile with the following content:

FROM ubuntu:18.04
ENTRYPOINT ["echo", "Hello World!"]

Build the Docker image and tag it:
docker build -t myhelloworld .

Confirm the image exists and view its history:
docker images
docker history myhelloworld
Run your new image:
docker run --name helloworld myhelloworld

10. Handy Tips and Shortcuts
To detach from an attached container session without stopping it, press Ctrl+P then Ctrl+Q.
Use docker rm --force to stop and remove containers in one step.
For quick experimentation, use docker run -it <image> bash.
""")
f1.close()

# LangChain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chat_models import ChatOpenAI
from langchain.chains import create_qa_with_sources_chain
from langchain.chains import ConversationalRetrievalChain

# Step 1
raw_documents = TextLoader("./example.txt").load()

# Step 2
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500, chunk_overlap=50, length_function=len
)
documents = text_splitter.split_documents(raw_documents)

# Step 3
embeddings_model = OpenAIEmbeddings()
# The vectordb object from the document
db = FAISS.from_documents(documents, embeddings_model)

# Step 4
retriever = db.as_retriever()

# Step 5
llm_src = ChatOpenAI(temperature=0, model="gpt-3.5-turbo-16k")
qa_chain = create_qa_with_sources_chain(llm_src)
retrieval_qa = ConversationalRetrievalChain.from_llm(
    llm_src,
    retriever,
    return_source_documents=True,
)

# Output
output = retrieval_qa({
    "question": "What is the purpose of o1 model?",
    "chat_history": []
})
print(f"Question: {output['question']}")
print(f"Answer: {output['answer']}")
print(f"Source: {output['source_documents'][0].metadata['source']}")


output = retrieval_qa({
    "question": "When was the o1 model previewed first?",
    "chat_history": []
})
print(f"Question: {output['question']}")
print(f"Answer: {output['answer']}")
print(f"Source: {output['source_documents'][0].metadata['source']}")


# Step 1
raw_documents2 = TextLoader("./dockertutorial.txt").load()

# Step 2
text_splitter2 = RecursiveCharacterTextSplitter(
    chunk_size=1000, chunk_overlap=100, length_function=len
)
documents2 = text_splitter2.split_documents(raw_documents2)

# Step 3
embeddings_model2 = OpenAIEmbeddings()
# The vectordb object from the document
db2 = FAISS.from_documents(documents2, embeddings_model2)

# Step 4
retriever2 = db2.as_retriever()

# Step 5
qa_chain2 = create_qa_with_sources_chain(llm_src)
retrieval_qa2 = ConversationalRetrievalChain.from_llm(
    llm_src,
    retriever2,
    return_source_documents=True,
)
# Output
output = retrieval_qa2({
    "question": "how to build a docker image with dockerfile",
    "chat_history": []
})
print(f"Question: {output['question']}")
print(f"Answer: {output['answer']}")
print(f"Source: {output['source_documents'][0].metadata['source']}")

output = retrieval_qa2({
    "question": "How to restart a stopped container",
    "chat_history": []
})
print(f"Question: {output['question']}")
print(f"Answer: {output['answer']}")
print(f"Source: {output['source_documents'][0].metadata['source']}")



