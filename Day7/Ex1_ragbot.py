# -*- coding: utf-8 -*-
"""Untitled30.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mYd3TjYq0pkNeF_wNarZ4fPq_eferbJk
"""
import os
os.environ["OPENAI_API_KEY"] = "REPLACE_YOUR_KEY_HERE"
import requests
url = "https://raw.githubusercontent.com/dsainatarajan/genai_infraops/refs/heads/main/Day7/DatasetCreationInput.txt"

res = requests.get(url)
with open("kubernetes_ref.txt", "w") as f:
  f.write(res.text)

# LangChain
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chat_models import ChatOpenAI
from langchain.chains import create_qa_with_sources_chain
from langchain.chains import ConversationalRetrievalChain

# Step 1
raw_documents = TextLoader("./DatasetCreationInput.txt").load()

# Step 2
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=5000, chunk_overlap=200, length_function=len
)
documents = text_splitter.split_documents(raw_documents)

# Step 3
embeddings_model = OpenAIEmbeddings()
# The vectordb object from the document
db = FAISS.from_documents(documents, embeddings_model)

# Step 4
retriever = db.as_retriever()

# Step 5
llm_src = ChatOpenAI(temperature=0, model="gpt-4o-mini")
qa_chain = create_qa_with_sources_chain(llm_src)
retrieval_qa = ConversationalRetrievalChain.from_llm(
    llm_src,
    retriever,
    return_source_documents=True,
)

# Output
output = retrieval_qa({
    "question": "Give me the kubectl command to create a deployment with httpd image?",
    "chat_history": []
})
print(f"Question: {output['question']}")
print(f"Answer: {output['answer']}")
print(f"Source: {output['source_documents'][0].metadata['source']}")


output = retrieval_qa({
    "question": "How to create a mysql deployment in kubernetes, give me all details",
    "chat_history": []
})
print(f"Question: {output['question']}")
print(f"Answer: {output['answer']}")
print(f"Source: {output['source_documents'][0].metadata['source']}")


while True:
    question = input("Give the next question to the AI: ")
    output = retrieval_qa({
        "question": question,
        "chat_history": []
    })
    print(f"Question: {output['question']}")
    print(f"Answer: {output['answer']}")
    print(f"Source: {output['source_documents'][0].metadata['source']}")
    # Get the next question in the loop
